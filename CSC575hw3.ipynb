{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c06194f-9982-4af9-a72a-70841399b013",
   "metadata": {},
   "source": [
    "#### Student Name: Mai Ngo\n",
    "#### Course Name and Number: CSC 575 Intelligent Information Retrieval - SEC 801\n",
    "#### Assignment 3 - Vector-space Retrieval and Evaluation\n",
    "#### Date: 2/4/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c04cd5-051c-4fb2-ba77-0806e0a1eafa",
   "metadata": {},
   "source": [
    "### Step 1: Load Inverted Index and compute Doc Vector Length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8038f5ea-3dfa-4a6a-8664-dad3c96a03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19822fc1-27d4-4d06-bd14-802bcefa74a6",
   "metadata": {},
   "source": [
    "#### Read the term index file and populate the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a2e39b-4726-478e-bdc9-f20daf5fa46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of docs in the corpus.\n",
    "N = 1033\n",
    "\n",
    "#Dictionary for inverted index {term as key | tuple(idf, list of postings) as value}.\n",
    "#idf: of a term in the entire document corpus. JUST THE IDF!!! -- \n",
    "#List of postings: element is individual dictionary {doc ID as keys | raw term frequency as value}.\n",
    "term_invIndex = {} \n",
    "\n",
    "#Dictionary {doc ID as key | vector length of it as value}.\n",
    "docID_lenDict = {} \n",
    "\n",
    "#Dictionary {term ID as key | term as value}\n",
    "termID_termDict = {}\n",
    "\n",
    "#Each word (string) to its term ID and document frequency - count of docs where the term appears.\n",
    "term_indexFile = open('medline_term_index.csv', 'r', encoding='utf-8')\n",
    "reader = csv.reader(term_indexFile, delimiter='\\t')\n",
    "\n",
    "#Extract individual columns.\n",
    "for line in reader:\n",
    "    term = line[0]              #term string\n",
    "    termID = line[1]            #termID\n",
    "    df = int(line[2])           #document frequency\n",
    "    idf = math.log10(N/df)      #idf\n",
    "    \n",
    "    #For each term, values as tuples (idf, empty dictionary)\n",
    "    term_invIndex[term] = (idf, dict())\n",
    "    #For each term ID, value as corresponding term \n",
    "    termID_termDict[termID] = term \n",
    "term_indexFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c7382-d7a8-4928-8a57-f42b3fe014b5",
   "metadata": {},
   "source": [
    "#### Read the inverted index file and add postings lists in term_invIndex dicitionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3675b50-e2b3-4c51-8bb6-fd4bd5bf9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # terms: 11463\n",
      " - Entry for 'pentobarbit': df=4, idf=2.412040330191658\n",
      " - Entry for 'defici': df=39, idf=1.4230357144931214\n",
      " - Entry for 'treatment': df=172, idf=0.7785718746120717\n",
      "\n",
      "Total # documents: 1033\n",
      " - Vector len for Doc 59 = 13.811725366348801\n",
      " - Vector len for Doc 1033 = 31.163653356034512\n"
     ]
    }
   ],
   "source": [
    "#Each term ID to positing lists (doc ID and count of term within).\n",
    "inv_indexFile = open('medline_inverted_index.csv', 'r')\n",
    "reader = csv.reader(inv_indexFile, delimiter='\\t')\n",
    "for line in reader:\n",
    "    termID = line[0]\n",
    "    idx = 1\n",
    "    while idx < (len(line)-1):\n",
    "        docID = line[idx] #A string\n",
    "        #Raw count of the term in this document, convert to integer. \n",
    "        termFreq = int(line[idx+1]) \n",
    "        \n",
    "        #Nested dictionary for each individual posting element {doc ID as key | raw termFreq as value}.\n",
    "        #postingDict is generated by using termID in termID_termDict, to get corresponding key termDict. \n",
    "        #Further, using that termDict to retrieve value from term_invIndex. \n",
    "        #Since that value was initialized as a tuple with 2nd element as a dictionary. \n",
    "        #Each posting will be deemed as individual postingDict. \n",
    "        #Keep creating till end of each line/row.\n",
    "        postingDict = (term_invIndex[termID_termDict[termID]])[1]\n",
    "        postingDict[docID] = termFreq  \n",
    "        \n",
    "        #Accumulate the component vector length for the document.\n",
    "        #Compute Sqrt tf_idf for each term in current document \n",
    "        tf_idf = termFreq * (term_invIndex[termID_termDict[termID]])[0] \n",
    "        tf_idfSq = math.pow(tf_idf, 2.0)\n",
    "        if docID in docID_lenDict:\n",
    "            docID_lenDict[docID] += tf_idfSq\n",
    "        else:\n",
    "            docID_lenDict[docID] = tf_idfSq\n",
    "        \n",
    "        #Increase for next postings within a row.\n",
    "        idx += 2\n",
    "inv_indexFile.close()\n",
    "\n",
    "#Normalized vector represents each single document in the corpus.\n",
    "for docID in docID_lenDict.keys():\n",
    "    #Take Sqrt of of the accumulated Sqrt of tf_idf\n",
    "    val = docID_lenDict[docID]\n",
    "    docID_lenDict[docID] = math.sqrt(val)\n",
    "\n",
    "    \n",
    "print ('Total # terms: %d' % len(term_invIndex))\n",
    "for term in ['pentobarbit', 'defici', 'treatment']:\n",
    "    print (' - Entry for \\'%s\\': df=%s, idf=%s' % (term, len(term_invIndex[term][1]), term_invIndex[term][0]))\n",
    "\n",
    "print ('\\nTotal # documents: %d' % len(docID_lenDict))\n",
    "for docID in ['59', '1033']:\n",
    "    print (' - Vector len for Doc %s = %s' % (docID, docID_lenDict[docID]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4502e4-c953-42db-8a67-4afdb28c7982",
   "metadata": {},
   "source": [
    "### Step 2: Store queries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d89ab55-d1b7-40ae-bf99-c40ea9587134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # queries: 30\n",
      " - Query 2: {'relationship': 1, 'blood': 1, 'cerebrospin': 1, 'fluid': 1, 'oxygen': 1, 'concentr': 1, 'partial': 1, 'pressur': 1, 'method': 1, 'interest': 1, 'polarographi': 1}\n",
      " - Query 22: {'mycoplasma': 1, 'infect': 1, 'presenc': 1, 'embryo': 1, 'fetu': 1, 'newborn': 1, 'infant': 1, 'anim': 1, 'pregnanc': 1, 'gynecolog': 1, 'diseas': 1, 'relat': 1, 'chromosom': 2, 'abnorm': 1}\n"
     ]
    }
   ],
   "source": [
    "#List of queries. Each element is a tuple (query ID, nested dictionary {term as key | term frequency as value}.\n",
    "#Term frequency: Total occurrence of a term in query corpus.\n",
    "queriesList = []\n",
    "\n",
    "#30 queries. query ID followed by the query as strings.\n",
    "queryFile = open('medline.query', 'r', encoding='utf-8')#'iso-8859-1'\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "for line in queryFile:\n",
    "    matchObj = re.match(r'^(\\d+)\\s+(.*)', line)\n",
    "    if not matchObj:\n",
    "        print (\"ERROR with line -- %s\" % line)\n",
    "    else:\n",
    "        queryID = matchObj.group(1) #queryID\n",
    "        text = matchObj.group(2)    #query string (ignore sentences)\n",
    "\n",
    "        #Process text string\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        terms = [porter.stem(w) for w in tokens if w not in stopwords.words('english') and len(w) > 1] # (**)\n",
    "        \n",
    "        #Dictionary term frequency. For each query, {term as key | frequency as value}. \n",
    "        fdist = nltk.FreqDist(terms)\n",
    "        \n",
    "        #Each element as tuple (queryID, term freq dictionary).\n",
    "        queriesList.append((queryID, dict(fdist)))\n",
    "queryFile.close()\n",
    "\n",
    "print ('Total # queries: %d' % len(queriesList))\n",
    "for queryID in [1, 21]:\n",
    "    print (' - Query %s: %s' % (queriesList[queryID][0], queriesList[queryID][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8e4e0-c13a-4721-a1a0-18c5250c64af",
   "metadata": {},
   "source": [
    "### Step 3: Inverted-Index Retrieval Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebecf304-c7fd-40e1-9ce3-9c38c5b8805f",
   "metadata": {},
   "source": [
    "#### Compute TF-IDF for the queries -- using IDF with respect to document corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185c0ebf-0f86-4a50-a4d4-e6e41c0fa31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First query TF-IDF:\n",
      "{'crystallin': 2.169002281505364, 'len': 1.401316464799885, 'vertebr': 2.412040330191658, 'includ': 1.1390390581279206, 'human': 0.957195470183148}\n"
     ]
    }
   ],
   "source": [
    "#Dictionary to store tf-idf of the queries. queryID as keys | nested dictionary term_tf_idf as values.\n",
    "#Respective value are a dictionary with term as keys and its tf-idf as value.\n",
    "tf_idfQueries = {}\n",
    "\n",
    "totalQueries = len(queriesList)\n",
    "\n",
    "#Nested loops to retrieve term frequency with respect to query corpus. \n",
    "#1st Iteration: Extract term dictionary for each query. \n",
    "#2nd Iteration: Extract terms (keys) from each term dictionary.\n",
    "#For each term, if found in other queries, add 1 to the accumulator sequence. \n",
    "#Sum to get total occurences in the query corpus.  \n",
    "termFrequencyDict = {term: sum(1 for qID, otherTerms in queriesList if term in otherTerms) for _, termDict in queriesList for term in termDict.keys()}\n",
    "\n",
    "#Iterate over each query in queriesList.\n",
    "for queryID, termDict in queriesList:\n",
    "    \n",
    "    tf_idfQueries[queryID] = {}\n",
    "\n",
    "    #Iterate over each term in current query.\n",
    "    for term, raw_tf in termDict.items():\n",
    "        #Check if the term exists in the document corpus vocabulary (term_invIndex)\n",
    "        if term in term_invIndex:\n",
    "            \n",
    "            #Retrieve the respective IDF. First element of the tuple.\n",
    "            termIDF = term_invIndex[term][0]\n",
    "\n",
    "            #TF-IDF = raw_tf (Query corpus) * IDF (Doc corpus)\n",
    "            tf_idfTerm = raw_tf * termIDF\n",
    "            tf_idfQueries[queryID][term] = tf_idfTerm\n",
    "\n",
    "print('First query TF-IDF:')\n",
    "print (tf_idfQueries['1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc113613-6095-4928-9055-f471cb016e14",
   "metadata": {},
   "source": [
    "#### Create a document-term scores dictionary --- WITH RESPECT TO A SINGLE DOCUMENT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf0196d-4de3-41c6-9b31-c38b45898ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary storing accumulated TF-IDF product scores.\n",
    "#{query ID as key | nested dictionary docID_termScore as value}.\n",
    "#Term ranking score: tf_idf of the term in query * its raw term frequency with respect to doc ID * its idf in doc corpus.\n",
    "doc_termScores = {}\n",
    "\n",
    "#Looking at each query term.\n",
    "for queryID, queryTerms in tf_idfQueries.items():\n",
    "    doc_termScores[queryID] = {}\n",
    "    for term, tf_idfQuery in queryTerms.items():\n",
    "        \n",
    "        #Retrieve term if available in doc corpus vocab.\n",
    "        if term in term_invIndex:\n",
    "            #Get idf in doc corpus and positing list.\n",
    "            termIDF_corpus = term_invIndex[term][0]\n",
    "            postingList = term_invIndex[term][1]\n",
    "            \n",
    "            #Get raw term frequency with respect to doc ID. \n",
    "            for docID, docTF in postingList.items():\n",
    "                \n",
    "                #If doc was not previously retrieved.\n",
    "                if docID not in doc_termScores[queryID]:\n",
    "                    doc_termScores[queryID][docID] = 0.0\n",
    "                    \n",
    "                doc_termScores[queryID][docID] += tf_idfQuery * docTF * termIDF_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec9353-b964-4946-b188-7154224468dc",
   "metadata": {},
   "source": [
    "### Step 4: Compute Cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8947b3-6715-463d-b154-b2e017e6b0c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute the length of the vector for the queries. -- Logic same as doc length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c336fa-32b9-494c-9a79-0b79660d4120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vector for each query:\n",
      "{'1': 3.8340357888582473, '2': 5.207790326731164, '3': 3.9297503247686905, '4': 3.5011867079842705, '5': 6.0027593875281084, '6': 3.6819295242023125, '7': 8.973309097542918, '8': 4.891318891084447, '9': 5.428657603192704, '10': 2.5098516927675893, '11': 4.128791494677564, '12': 4.797046938429461, '13': 4.778762493538398, '14': 7.307060337836872, '15': 6.344771177441073, '16': 5.9451508345390875, '17': 6.059060370294427, '18': 3.2088899359441747, '19': 3.3651122913770832, '20': 10.349813675699584, '21': 2.8471593353732514, '22': 6.170576471189581, '23': 2.3520320860722372, '24': 6.519564352044149, '25': 7.119969548504723, '26': 3.388234739891549, '27': 15.425305388401375, '28': 4.424085006061352, '29': 11.327630105367925, '30': 6.006551795231742}\n"
     ]
    }
   ],
   "source": [
    "#Dictionary {query ID as key | vector length of it as value}.\n",
    "query_lenDict = {} \n",
    "\n",
    "#Iterate over each query in tf_idfQueries\n",
    "for queryID, termScores in tf_idfQueries.items(): \n",
    "    \n",
    "    #Accumulator for each individual Sq of the TF-IDF value within a query.\n",
    "    query_tf_idfSq = 0.0\n",
    "\n",
    "    #Iterate over each term in the query.\n",
    "    for term, tf_idfTerm in termScores.items():\n",
    "        query_tf_idfSq += math.pow(tf_idfTerm, 2.0)\n",
    "\n",
    "    #Take square root of of the accumulated Sq of term tf_idf\n",
    "    query_lenDict[queryID] = math.sqrt(query_tf_idfSq)\n",
    "    \n",
    "print('Length of the vector for each query:')\n",
    "print(query_lenDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d892832-a6f9-4874-8405-5c7cdaa1ec6c",
   "metadata": {},
   "source": [
    "#### Compute Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62df4d6-3401-47e7-ad65-acee1e499349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary {query ID as key | nested dictionary as value}.\n",
    "#Nested dictionary: {doc ID as keys | cosine similarity as values}.\n",
    "cosineSim_scoreDict = {}\n",
    "\n",
    "#Each query, get vector length. \n",
    "for queryID in doc_termScores.keys():\n",
    "    queryLength = query_lenDict.get(queryID, 0.0)\n",
    "    cosineSim_scoreDict[queryID] = {}\n",
    "    \n",
    "    #Each doc, get vector length.\n",
    "    for docID, doc_termScore in doc_termScores[queryID].items():\n",
    "        docLength = docID_lenDict.get(docID, 0.0)\n",
    "        \n",
    "        #Check if lengths are greater than 0, denominator cannot be zero.\n",
    "        if queryLength > 0 and docLength > 0:\n",
    "            \n",
    "            #Cosine similarity score.\n",
    "            cosineSim = doc_termScore / (queryLength * docLength)\n",
    "            cosineSim_scoreDict[queryID][docID] = cosineSim\n",
    "        else:\n",
    "            cosineSim_scoreDict[queryID][docID] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0639a2-3dd5-4f33-9496-af3aec6a14de",
   "metadata": {},
   "source": [
    "### Step 5: Sort scores in descending order and write output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43315c-2866-4779-bfec-b2ccd46f4774",
   "metadata": {},
   "source": [
    "#### Sort scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71020065-3923-43ea-9061-c9ecf0484004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 [#doc=223] - FIRST 17 matched documents: \n",
      "\n",
      "{'72': 0.3661556461202483, '500': 0.3159256249993626, '965': 0.26138210670241724, '360': 0.20695320831826586, '171': 0.15505750411543895, '15': 0.15257626868366772, '166': 0.15165416970242887, '181': 0.14376422271927988, '513': 0.14197769739495128, '511': 0.12042775273226675, '184': 0.11523683306765448, '13': 0.11434807620744149, '167': 0.11398234525209128, '212': 0.11004970616680217, '138': 0.10915797336220515, '79': 0.10321366011991648, '509': 0.09994691727676606}\n"
     ]
    }
   ],
   "source": [
    "for queryID, docScores in cosineSim_scoreDict.items():\n",
    "    \n",
    "    sorted_docScores = sorted(docScores.items(), key=lambda x: x[1], reverse=True)\n",
    "    cosineSim_scoreDict[queryID] = dict(sorted_docScores)\n",
    "\n",
    "#Showing results like what you posted in the discussion. \n",
    "print(f\"Query 1 [#doc={len(cosineSim_scoreDict['1'])}] - FIRST 17 matched documents: \\n\")\n",
    "print(f\"{dict(islice(cosineSim_scoreDict['1'].items(),17))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e777f4-dd49-48fa-86ed-7112c25b8622",
   "metadata": {},
   "source": [
    "#### Re-organized ranked score. Excluding term scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7e27e6-db82-4426-8e11-794a68ba58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each query ID, list of ranked docs. \n",
    "docRank = {}\n",
    "\n",
    "for queryID, docScores in cosineSim_scoreDict.items():\n",
    "    docIDs = list(docScores.keys())\n",
    "    docRank[queryID] = docIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6d5b1-ef75-4ecd-b8ec-8742f7095175",
   "metadata": {},
   "source": [
    "#### Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4aff49-31f5-4d8f-aab7-ad7793a7e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written!\n"
     ]
    }
   ],
   "source": [
    "with open ('rankedlist.txt', 'w') as outFile:\n",
    "    for queryID, docIDs in docRank.items():\n",
    "        #Print template.\n",
    "        docIDs_template = ', '.join(docIDs)\n",
    "        outFile.write(f\"{queryID}, {docIDs_template}\\n\")\n",
    "\n",
    "print('Successfully written!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f34b8-d1c6-4e5b-b642-fc296d43c5d6",
   "metadata": {},
   "source": [
    "### Evaluation metric Mean Average Precision -- Compute MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219c6da-88b7-4752-b4d9-7ada5142d826",
   "metadata": {},
   "source": [
    "#### Open both files -- USE DICTIONARY due to different row length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08a4f318-a6fb-4a98-85a1-c60a5e726e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(fileName):\n",
    "    '''Read both files as dicitonaries {query ID as key | doc ID as value}.\n",
    "    Seperated by white space for medline.rel; by (', ') for rankedlist.txt.'''\n",
    "    \n",
    "    dataDict = {}\n",
    "    if fileName == 'medline.rel':\n",
    "        with open(fileName, 'r') as inFile:\n",
    "            for line in inFile:\n",
    "                parts = line.split()\n",
    "                queryID = parts[0]\n",
    "                docIDs = list(parts[1:])\n",
    "                dataDict[queryID] = docIDs\n",
    "    else: \n",
    "        with open(fileName, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.split(', ')\n",
    "                queryID = parts[0]\n",
    "                docIDs = [docID.strip() for docID in parts[1:]]  \n",
    "                dataDict[queryID] = docIDs\n",
    "    \n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85135e51-6aae-4fde-8a87-fc675ad51b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st query - medLine.rel:\n",
      "['13', '14', '15', '72', '79', '138', '142', '164', '165', '166', '167', '168', '169', '170', '171', '172', '180', '181', '182', '183', '184', '185', '186', '211', '212', '499', '500', '501', '502', '503', '504', '506', '507', '508', '510', '511', '513']\n"
     ]
    }
   ],
   "source": [
    "evalDict = readFile('medline.rel')\n",
    "\n",
    "print('1st query - medLine.rel:')\n",
    "print(f\"{evalDict['1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de98a17-5d79-45ab-b014-56537be347d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st query - rankedlist.txt:\n",
      "['72', '500', '965', '360', '171', '15', '166', '181', '513', '511', '184', '13', '167', '212', '138', '79', '509', '182', '512', '168', '170', '185', '164', '169', '172', '838', '175', '142', '14', '336', '186', '637', '180', '499', '986', '211', '645', '165', '183', '403', '401', '727', '506', '58', '177', '899', '549', '763', '256', '65', '570', '504', '11', '873', '206', '173', '231', '869', '99', '510', '125', '540', '501', '178', '127', '876', '640', '896', '542', '870', '404', '9', '209', '578', '758', '900', '913', '503', '654', '41', '174', '215', '112', '464', '213', '603', '816', '445', '642', '156', '163', '507', '383', '685', '333', '590', '981', '262', '660', '541', '584', '130', '259', '950', '744', '442', '469', '220', '875', '253', '508', '658', '835', '648', '606', '374', '700', '372', '589', '234', '516', '730', '567', '734', '160', '78', '643', '86', '248', '505', '619', '593', '849', '454', '342', '650', '826', '82', '87', '880', '375', '726', '42', '496', '886', '4', '195', '652', '67', '114', '863', '1001', '214', '521', '158', '1029', '267', '367', '577', '189', '585', '655', '300', '24', '502', '831', '573', '743', '108', '632', '984', '719', '162', '627', '486', '421', '207', '299', '888', '229', '999', '797', '905', '576', '800', '75', '631', '580', '872', '176', '83', '196', '466', '812', '909', '910', '188', '100', '769', '208', '664', '809', '95', '604', '322', '35', '480', '811', '70', '581', '485', '408', '326', '715', '927', '799', '557', '907', '282', '1020', '608', '569', '732']\n"
     ]
    }
   ],
   "source": [
    "rankedDict = readFile('rankedlist.txt')\n",
    "\n",
    "print('1st query - rankedlist.txt:')\n",
    "print(f\"{rankedDict['1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b98b8f-532b-4212-bdad-6830e4e03c18",
   "metadata": {},
   "source": [
    "#### Average Precision for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89466e77-cf5c-4d6b-9c18-ddb0cb56a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgPrecision_perQuery (rankedDict, evalDict):\n",
    "    '''Inputs two dictionaries: computed rank, and relevance for evaluation.'''\n",
    "    '''Return average precision score for each query.'''\n",
    "    \n",
    "    #Dictionary {query ID as key | respective average precision as value}.\n",
    "    avgPrecision_dict = {}\n",
    "    \n",
    "    for queryID, docID_list in rankedDict.items():\n",
    "            precisionK = 0.0\n",
    "            relevantCount = 0\n",
    "            for k, docID in enumerate(docID_list, start=1):\n",
    "                if docID in evalDict[queryID]:\n",
    "                    \n",
    "                    relevantCount += 1\n",
    "                    precisionK += relevantCount / k\n",
    "            \n",
    "            avgPrecision = precisionK / relevantCount\n",
    "            #print(queryID, avgPrecision)\n",
    "            avgPrecision_dict[queryID] = avgPrecision\n",
    "    return avgPrecision_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "649adf10-5a8d-49e7-9953-eb2a72104e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision for each query:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.7293854808442106,\n",
       " '2': 0.4289898694562347,\n",
       " '3': 0.5870782588251162,\n",
       " '4': 0.3185516515673878,\n",
       " '5': 0.774593708518988,\n",
       " '6': 0.7319987566685877,\n",
       " '7': 0.6193916474616104,\n",
       " '8': 0.45981430849851906,\n",
       " '9': 0.48646588164225285,\n",
       " '10': 0.575987465693348,\n",
       " '11': 0.4864665479828646,\n",
       " '12': 0.5759222949615418,\n",
       " '13': 0.9182293755070582,\n",
       " '14': 0.6324086048535709,\n",
       " '15': 0.5462439920811397,\n",
       " '16': 0.5810905131283283,\n",
       " '17': 0.2988271045816611,\n",
       " '18': 0.474891720090341,\n",
       " '19': 0.484011394055033,\n",
       " '20': 0.2203353933993955,\n",
       " '21': 0.3556434832069594,\n",
       " '22': 0.27928007125783366,\n",
       " '23': 0.8676803142707383,\n",
       " '24': 0.768025067969173,\n",
       " '25': 0.7244897511812423,\n",
       " '26': 0.5337814635400168,\n",
       " '27': 0.5549464206513848,\n",
       " '28': 0.5420939191163882,\n",
       " '29': 0.8027244937762165,\n",
       " '30': 0.6104667011732228}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgPrecision_dict = avgPrecision_perQuery(rankedDict, evalDict)\n",
    "print('Average precision for each query:')\n",
    "avgPrecision_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f26a5-d65b-4eb4-b496-1d46c8df7b49",
   "metadata": {},
   "source": [
    "#### Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee37e8a8-c254-4e89-b00c-4d40599f9d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP score: 0.5656605218653455\n"
     ]
    }
   ],
   "source": [
    "mapScore = sum(avgPre_query for avgPre_query in avgPrecision_dict.values())/len(avgPrecision_dict)\n",
    "print(f'MAP score: {mapScore}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055f071-e979-4342-b723-1e6c5432016b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
