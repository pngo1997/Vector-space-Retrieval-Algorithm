# ğŸ” Vector-space Retrieval & Mean Average Precision Evaluation  

## ğŸ“œ Overview  
This project implements the **Inverted Index Retrieval Algorithm** and evaluates retrieval performance using **Mean Average Precision (MAP)** on the **Medline Collection**, a benchmark dataset for Information Retrieval (IR) research.  

ğŸ“Œ **Dataset: Medline Collection**  
- **1,033 documents** (medical abstracts)  
- **30 queries**  
- **Relevance judgments** for query-document pairs  

ğŸ“Œ **Goals:**  
1. Implement **query-based document ranking** using the **Vector Space Model (VSM)**.  
2. Compute **cosine similarity** between queries and documents.  
3. Generate a **ranked list** of relevant documents.  
4. Evaluate retrieval effectiveness using **Mean Average Precision (MAP)**.  

ğŸ“Œ **Programming Language**: `Python 3`  
ğŸ“Œ **Libraries Used**: `NumPy`, `pandas`, `math`, `csv`  

## ğŸš€ Implementation Steps  

### **1ï¸âƒ£ Load Index & Query Data**  
- Read and store **inverted index (`H_invindex`)** mapping **terms â†’ (idf, postings)**.  
- Compute **document vector lengths (`DL_doclen`)** for normalization.  
- Load **queries (`Queries_list`)** as **(query ID, term frequency map)**.  

### **2ï¸âƒ£ Compute Query Vector Lengths**  
- Calculate **query vector lengths (`L`)** using term frequencies and **inverse document frequencies (IDF)**.  
- Skip query terms **not found** in the vocabulary.  

### **3ï¸âƒ£ Cosine Similarity & Ranking**  
- Compute **cosine similarity** between **query and document vectors**.  
- Rank documents **by descending similarity** for each query.  
- Store ranked results in **`rankedlist.txt`**.  

### **4ï¸âƒ£ Evaluate Performance (MAP Calculation)**  
- Compare ranked results against **relevance judgments (`medline.rel`)**.  
- Compute **precision at each relevant document rank**.  
- Calculate **Mean Average Precision (MAP)** across all queries.  
